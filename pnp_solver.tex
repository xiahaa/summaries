\documentclass[a4paper]{article}
\usepackage[left=3cm, right=3cm]{geometry}
\usepackage{lipsum}
\usepackage{tikzpagenodes}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,matrix}
\pgfplotsset{compat=1.8}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage[colorlinks=true,citecolor=green]{hyperref}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{url}
\usepackage{cite}
\usepackage{bm}
\usepackage{pbox}
\usepackage{siunitx,booktabs,etoolbox}
\usepackage{ulem}

%\usepackage{pgf,tikz,pgfplots}
%\pgfplotsset{compat=1.15}
%\usepackage{mathrsfs}

\usetikzlibrary{arrows}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\title{Pose Estimation: $3$D to $2$D case}
\author{Xiao Hu, emails: \url{xiahaa@space.dtu.dk}}
\maketitle
\begin{figure}
\centering
\includegraphics[scale=0.8]{hand_eye_files/vision/figures/pnp.pdf}
\caption{Principles of the Perspective $n$ Point problem.}
\end{figure}
\section{Introduction}
The Perspective-$n$-Point (P$n$P) problem is a fundamental problem in geometric computer vision. Given a certain number ($n$) of correspondences between 3D world points and 2D image measurements, the problem consists of fitting the absolute position and orientation of the camera to the measurement data~\cite{fischler1981random}. A considerable number of approaches have been proposed for the past twenty years. Generally speaking, they can be divided according to different characteristics:
\begin{itemize}
\item By whether or not the solver is run iteratively or not, Iterative V.S. Non-iterative.
\item By the require number of correspondence, P$3$P, P$4$P and more generally, P$n$P.
\end{itemize}

In my opinion, P$3$P and scalable P$n$P would be our focus since other methods are kind of intermediate products of the developing procedure. The reason why P$3$P is still favoured especially after the invention of efficient P$n$P algorithm is as follows:
\begin{itemize}
\item P$3$P is the minimal number of correspondences required to solve the general P$n$P problem. It should be noted that by adding some constraints embedded by the rigid body, this number can go down to $1$, e.g. for non-holonomic vehicle.
\item Although P$n$P algorithm can use the data redundancy to suppress the influence of noises, it can not deal with outliers. The most famous way to deal with outliers is to cascade the P$n$P algorithm with a RANSAC routine. Since for RANSAC, we prefer to use the minimal set to solve a given problem. P$3$P shows its superiority in this sense. 
\end{itemize}
The first P$3$P algorithm in a geometric view is given in~\cite{fischler1981random}, later improvements have been addressed in~\cite{pop00011},~\cite{gao2003complete}, ~\cite{li2011stable} and~\cite{kneip2011novel}.

Scalable P$n$P algorithm means the complexity of the P$n$P algorithm is $\mathcal{O}(n)$, which means its complexity or computational time grows linearly with the number of correspondence. This is very important since we would like to benefit from data redundancy without paying too much cost. The earlier P$n$P algorithms, like~\cite{ansar2003linear} ($\mathcal{O}(n^8)$) and~\cite{quan1999linear} ($\mathcal{O}(n^4)$), are not traceable if we are going to work with thousands of correspondences. EPnP~\cite{lepetit2009epnp}~\cite{moreno2007accurate} is the first P$n$P algorithm with $\mathcal{O}(n)$ complexity, followed by RPnP~\cite{li2012robust}, DLS~\cite{hesch2011direct}~\cite{nakano2015globally}, AsPnP~\cite{zheng2013aspnp}, OPnP~\cite{zheng2013revisiting}, UPnP~\cite{kneip2014upnp}.

Among iterative solutions, LHM~\cite{lu2000fast} is the most famous solution. Other solutions also exist, like RPPnP~\cite{garro2012solving} and the recently published~\cite{8470970}. Iterative solution usually show superior performance under noisy conditions. However, it is easy to see that their computational burden grows at least linearly with the number of correspondences. One of the biggest disadvantage of those iterative solutions is that the global optimality cannot be guaranteed since mathematically speaking, it is a nonlinear optimization problem which is nonconvex.

In this sense, ~\cite{schweighofer2008globally} proposed a method based on Semi-Definite Programming (SDP) and Semi-Definite Relaxation (SDR). Unfortunately, after SDR, the global optimality cannot always be retained to the original problem. Moreover, solving a SDP is usually time-consuming or at least not in real-time.

With regarding to probabilistic optimality, several methods have been proposed to solve this problem in the sense of Maximum-Likelihood Estimation (MLE)~\cite{ferraz2014leveraging},~\cite{urban2016mlpnp}. Currently, I have no plan to detail this kind of solutions. But the principle is to reformulate a cost function in order to maximize the likelihood and then solve this optimization problem iteratively. Also a reasonable estimation of the prior covariance should be built.

Planar case is one of the special case where multiple minimums can exist and hard to distinguish. Similar to~\cite{rpp}, modern approaches may provide more than one solutions if ambiguity exists. This is one point we need to be aware of. The selection of the correct result can be done with the help of other sensors or motion dynamics.

It is also worthy to mention the famous POSIT algorithm which works for object-to-image registration problem~\cite{dementhon1995model}~\cite{oberkampf1996iterative}. It is a iterative algorithm work under the assumption of weak orthogonal projection. An further improvement which allows POSIT works in unknown correspondence condition was published in~\cite{softposit}.

\textcolor{blue}{All those aforementioned algorithms have no capability of dealing with outliers themselves. RANSAC is needed in order to filter out outliers. The conventional routine for outlier removal functions like: 
\begin{itemize}
\item random select a set of correspondences.
\item apply a algorithm and testify the consensus.
\item iterate this until a certain number of iteration or inlier percentage is high enough.
\item refinement with all inliers.
\end{itemize}
This conventional routine is sometimes time-consuming since solving P$n$P and verification (usually based on reprojection error) needs to be carried out in each iteration.}
\textcolor{red}{So this is a still quite open area.} Recent work by~\cite{ferraz2014very} proposes a very fast outlier removal technique. We will detail this in the following section.

Other related topics include: solving P$n$P problem with other intrinsics unknown, such as focus length and radial distortion~\cite{pop00010}~\cite{zheng2016direct}; solving P$n$P problem with even less correspondence with the help of other constraints and sensors~\cite{pylvanainen2009revisiting},~\cite{d2014use},~\cite{d2013use}; continuous solving the P$n$P problem with observers~\cite{pop00016}; over-parameterization~\cite{pop00018}.

\section{Theory and Implementation}
There are two kind of objective functions for the P$n$P problem:

The first one is aiming to minimize the image error or the algebraic error:
\begin{align*}
f =& \underset{\mathbf{R},\ \mathbf{t}}{argmin} \sum_{i=1}^{N}||\mathbf{q}_i-\pi(\mathbf{R}\mathbf{P_i}+\mathbf{t})||^2 \Leftrightarrow \\
 &\underset{\mathbf{R},\ \mathbf{t}}{argmin} \sum_{i=1}^{N}\left((\frac{q_1}{q_3}-\frac{\mathbf{R}_(1,:)\mathbf{P_i}+t_1}{\mathbf{R}_(3,:)\mathbf{P_i}+t_3})^{2}+(\frac{q_2}{q_3}-\frac{\mathbf{R}_(2,:)\mathbf{P_i}+t_2}{\mathbf{R}_(3,:)\mathbf{P_i}+t_3})^{2}
 \right)
\end{align*}
where $\pi$ represents the projective function.

Another function is aiming to minimize the object-space error which is the alignment error of the back-tracing ray of the pixel and the ray from camera center to object points in camera coordinate frame:
\begin{align*}
f =& \underset{\mathbf{R},\ \mathbf{t}}{argmin} \sum_{i=1}^{N}||(\mathbf{I}-\mathbf{V}_i)(\mathbf{R}\mathbf{P_i}+\mathbf{t})||^2,\ with\ \mathbf{V}_i = \frac{\mathbf{q_iq_i}^T}{\mathbf{q_i}^T\mathbf{q_i}}
\end{align*}
\textbf{Note:} we assume here $\mathbf{q}_i$ has been normalized ($\mathbf{q}_i = \mathbf{K}^{-1}\mathbf{q}_i'$).

Generally speaking, the P$n$P problem is to find the answer for the aforementioned two optimization problem. What makes this problem challenging is the rotation matrix $\mathbf{R}\in \mathbb{SO}(3)$ which will implicitly impose nonlinear, nonconvex constraints. This is why the famous Direct-Linear-Transformation (DLT)\footnote{DLT means we omit those constraints and take each elements of $\mathbf{R}$ as separate variables. This can make the objective function as well as the constraints in affine or quadratic form.} method cannot given us a promising result.

Here are some useful techniques which will help us solve this problem~\cite{hartley2007optimal}:
\begin{itemize}
\item Branch-and-Bounding;
\item Convex optimization, such as Second-Order-Cone-Programming (SOCP), SDP: this usually requires relaxation or even solve the prime problem by solving the dual problem;
\item Algebraic Geometry: use Gr\"{o}ebner basis. This is feasible since the objective function and the constraints can be formulate as a polynomial system and the answer is the common root that makes those polynomials vanish~\cite{kukelova2008automatic}. 
\item Manifold Optimization which applies optimization directly on $\mathbb{SO}(3)$ and $\mathbb{SE}(3)$.
\end{itemize}

\subsection{P$3$P}
We start with the P$3$P algorithms. Generally speaking, the P$3$P problem solves this problem by firstly estimating the arc-length of each points in the camera frame, then reconstructs the point cloud in the camera frame by multiplying the arc-length with the back-tracing rays, and finally applies the $3$D to $3$D pose estimation routines in order to find the $\mathbf{R}$ and $\mathbf{t}$.
\begin{figure}
\centering
\includegraphics[scale=0.8]{hand_eye_files/vision/figures/p3p.pdf}
\caption{Principles of the Perspective $3$ Point problem.}
\label{fig:p3p}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.5]{hand_eye_files/vision/figures/p3p_kneip.png}
\caption{Figure from~\cite{kneip2011novel}.}
\label{fig:p3p}
\end{figure}
From Fig~\ref{fig:p3p}, by denoting $|CP_1|=d1,\ |CP_2|=d2,\ |CP_3|=d3,\ |P_1P_2|=d12,\ |P_1P_3|=d13,\ |P_2P_3|=d23,\ $, we will have the following relationship using the cosine rule:
\begin{align*}
d12^2 = d1^2+d2^2-2cos(\alpha_2)d1d2 \\
d13^2 = d1^2+d3^2-2cos(\alpha_1)d1d3 \\
d23^2 = d2^2+d3^2-2cos(\alpha_3)d2d3 \\
\end{align*}
where $\alpha_1 = acos(\mathbf{q}_1 \cdot \mathbf{q}_3),\ \alpha_2 = acos(\mathbf{q}_1 \cdot \mathbf{q}_2),\ \alpha_3 = acos(\mathbf{q}_2 \cdot \mathbf{q}_3)$. By some algebraic operations\footnote{eliminate one variable by the other two and then continue}, we will have a $4^{th}$ order polynomial equation:
\begin{align*}
A_4 t^4+A_3 t^3+A_2 t^2+A_1 t^1+A_0 = 0
\end{align*}
By solving this equation, we will have the arc-length. Continue this for other arcs, we can find them all. By multiplying $\mathbf{q}_i$ with $d_i$, we will have $\mathbf{Q}_i$, then the problem transforms to:
\begin{align*}
f =& \underset{\mathbf{R},\ \mathbf{t}}{argmin} \sum_{i=1}^{N}||\mathbf{Q}_i-\mathbf{R}\mathbf{P_i}+\mathbf{t}||^2
\end{align*}
which is a typical $3$D to $3$D pose estimation problem that can be solved.

P$3$P methods~\cite{fischler1981random},~\cite{gao2003complete}, and~\cite{li2011stable} all function in this way. ~\cite{fischler1981random} is more like a hard-code solution.~\cite{gao2003complete} gives detailed illustration of the P$3$P problem and also their solution under some degenerate cases. However,~\cite{gao2003complete} is too mathematical which is hard to read. But if you only care about the solution, then you can go directly to the normal case.~\cite{li2011stable} is the most clear paper that clearly explain this problem.


The only different P$3$P method was proposed in~\cite{kneip2011novel}. By introducing two intermediate frames, it changes the original $3$D to $2$D problem to a plane problem. After geometric operations, it finally results in a $4^{th}$ order polynomial equation. However, this $4^{th}$ order polynomial does not take arc-length as the variable, but an alternative parameter which represent the tilt angle of the $P_1-P_2-C$ plane. So the final $\mathbf{R},\ \mathbf{t}$ have to be found in another way rather than solving the $3$D to $3$D pose estimation problem. \textcolor{blue}{This method is easy to understand and very fast.}

\subsubsection{Summary of P$3$P}
Here are several points which remains unclear:
\begin{itemize}
\item which P$3$P gives the best performance in terms of stability, runtime?
\item how can P$3$P benefit from data redundancy? Or P$3$P can only be used for outlier removal together with RANSAC?
\end{itemize}

\subsection{Complex P$n$P}
Here, Complex P$n$P means not-well scalable P$n$P algorithms such as~\cite{quan1999linear} and~\cite{ansar2003linear}. I will begin with~\cite{quan1999linear} and then talk about~\cite{ansar2003linear}.

The linear P$n$P in~\cite{quan1999linear} is based on the cosine rule function we have seen in P$3$P.~\cite{quan1999linear} uses the Sylvester resultant to eliminate variable, which will result in a $8^{th}$ order polynomial which only contains even terms (equals to a $4^{th}$ order polynomial). The idea is quite straightforward: since each tuple-3, e.g ${1,2,3} or {1,4,5}$, will generate a $4^{th}$ order polynomial in, e.g. $d_1$. So, $N$ points generates $C_{N}^{2}$ cosine equations which furthermore generates $\frac{(n-1)(n-2)}{2}$ $4^{th}$ order polynomials (eliminating redundant polynomials). By taking the linearization which means taking $\mathbf{t}=(1,x,x^2,x^3,x^4)^{T}$ as separate terms, we will have a equation like $\mathbf{A}\mathbf{t}=0$. 
$$
\underbrace{
\left[
\begin{matrix}
\mathbf{a}_1 \\
\mathbf{a}_2 \\
\vdots \\
\mathbf{a}_K \\
\end{matrix}
\right]}_{\mathbf{A}}
\left[
\begin{matrix}
1 \\ x \\ x^2 \\ x^3 \\ x^4
\end{matrix}
\right]=\mathbf{0}
$$
Then $\mathbf{t}$ can be found by taking the right singular vector. Supposing the dimension of the null space is over one, which means 
$$
x = \lambda \mathbf{v}_4 + \beta \mathbf{v}_5, \lambda, \beta \in \mathbb{R}.
$$
In order to solve for $\lambda, \beta$, we should use the so-called re-linearization technique:
Notice that $t_it_j=t_kt_l,\ if\ i+j=k+l$, so we call formulate another normal system with $(\lambda^2, \lambda\beta, \beta^2$. We solve again for $\lambda, \beta$ and then for $d_i$. The final pose estimation problem, once again, turns out to be a $3$D to $3$D pose estimation problem. \textcolor{blue}{As can be seen from here, data redundancy is utilized by stacking the $4^{th}$ order polynomial equation and find the null vector. However, since the aforementioned procedure has to be run for each possible arc, for sure, the complexity does not scale very well.}

Another linear P$n$P algorithm is proposed in~\cite{ansar2003linear}. It also starts with the cosine equations. However, they applied linearization as follows:
\begin{align*}
&\text{recall: } x_1^2+x^2-2x_1x_2cos(\alpha_{12})-d_{12}^2=0 \\
&\text{linearization: $\mathbf{t}={x_1^2, x_1x_2, x_2^2}$} \\
&\text{do this for $N$} \Leftrightarrow \\
&\underbrace{
\left[
\begin{matrix}
-2cos(\alpha_{12}) & 0 & \cdots & 0 \\
0 & -2cos(\alpha_{13}) & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & -2cos(\alpha_{n, n-1})\\
\end{matrix}
\right.
\left|
\begin{matrix}
1 & 1 & 0 & \cdots & 0 & 0 & -d_{12}^2\\
1 & 0 & 1 & \cdots & 0 & 0 & -d_{13}^2\\
\vdots & \vdots & \vdots& \ddots & \ddots & vdots & vdots \\
0 & 0 & \cdots & cdots & 1 & 1 & -d_{n, n-1}^2\\
\end{matrix}
\right]
}_{\mathbf{M}} \\
&\mathbf{M}\mathbf{t} = \mathbf{0}
\end{align*}
Finding $\mathbf{t}$ by choosing the rightmost singular vector and apply a similar relinearization to recover $x$. For $N$ points, we have $C_{N}^2$ cosine equations. Then we have $N$ terms in form of $x_i^2$, $C_{N}^2$ terms in form of $x_ix_j$ and one slack variable $1$. So in total, the solution vector is $\frac{N(N+1)}{2}+1$. The kernel is of dimension $\frac{N(N-1)}{2}$, so we will have a null space of dimension $N+1$. We still needs $\frac{N*N*(N-1)}{2}$ constraints to do the relinearization. As can be seen, this method is extremely complex. The computation time will increase significantly with the number of correspondences. \textcolor{red}{This method is only theoretically meaningful, but cannot be used in practice, especially now we have the $\mathcal{O}(n)$ solutions.}

\subsection{Summary of Complex P$n$P}
The linearization and relinearization techniques opens the gate for later $\mathcal{O}(n)$ P$n$P solutions. Regard to the Complex P$n$P, we could include them as the related work, but use other $\mathcal{O}(n)$ solutions in practice.

\subsection{POSIT \& SoftPOSIT}


SoftPOSIT is an improvement for orginal POSIT algorithm which aims to estimate pose and correspondence simultaneuously. As can be seen from this statement, for SoftPOSIT solution, the correspondence could be unknown or partially known. This serves the biggest difference of SoftPOSIT algorithm relative to other solutions mentioned in this report.
\subsubsection{POSIT routine}
Assume object points are  $\mathbf{P}_i,\ i=1,\ \cdots,\ M$ (homogeneous coordiantes) and image points $\mathbf{q}_j,\ j=1,\ \cdots,\ N$ (normalized homogeneous coodrinates, so $f=1$), then the coordinates of object points relative to an origin $\mathbf{P}_0$ can be represented as $\tilde{\mathbf{P}}_i=\mathbf{P}_i-\mathbf{P}_0$. From the perspective projection model, we have:
\begin{align*}
w\left[
\begin{matrix}
x \\ y \\ 1
\end{matrix}\right]&=\left[
\begin{matrix}
\mathbf{R}_1^T & tx \\ 
\mathbf{R}_2^T & ty \\ 
\mathbf{R}_3^T & tz \\ 
\end{matrix}
\right]
\left[
\begin{matrix}
\tilde{\mathbf{P}} \\ 1
\end{matrix}
\right] \\
w\left[
\begin{matrix}
x \\ y 
\end{matrix}\right]&=s
\left[
\begin{matrix}
\mathbf{R}_1^T & tx \\ 
\mathbf{R}_2^T & ty \\ 
\end{matrix}
\right]
\left[
\begin{matrix}
\tilde{\mathbf{P}} \\ 1
\end{matrix}
\right] \\
with\ s=&\frac{1}{t_z},\ w=\frac{\mathbf{R}_3^T\tilde{\mathbf{P}}_i}{tz}+1
\end{align*}
If $t_z >> \mathbf{R}_3^T\tilde{\mathbf{P}}_i$, then $w \approx 1$. Furthermore, we will have 
\begin{align*}
\left[
\begin{matrix}
x \\ y 
\end{matrix}\right]&=s
\left[
\begin{matrix}
\mathbf{R}_1^T & tx \\ 
\mathbf{R}_2^T & ty \\ 
\end{matrix}
\right]
\left[
\begin{matrix}
\tilde{\mathbf{P}} \\ 1
\end{matrix}
\right]
\end{align*}
which defines the scaled orthographic projection. Taking the transpose of this equation, we will have
\begin{align*}
[X,\ Y,\ Z,\ 1]
\underbrace{\left[
\begin{matrix}
s\mathbf{R}_1 & s\mathbf{R}_2 \\ 
stx & sty \\ 
\end{matrix}
\right]}_{\mathbf{Q}}=[wx,\ wy]
\end{align*}
$\mathbf{Q}$ is the variable we are going to optimize. Once we have $\mathbf{Q}$, we can extract the corresponding blocks as $s\mathbf{R}_1$ and $s\mathbf{R}_2$, since we know that $||\mathbf{R}_1||=1$, $s$ can be recorvered as $s=\sqrt{|s\mathbf{R}_1||s\mathbf{R}_2|}$. Then $\mathbf{R}_3=\mathbf{R}_1 \times \mathbf{R}_2$, $t_x = \frac{st_x}{s},\ t_y=\frac{st_y}{s}, t_z=\frac{1}{s}$. This process runs iteratively by firstly letting $w_k=1,\ k=1,\ 2,\ \cdots,\ M$ and update $w_k$ with newly estimated $\mathbf{R}, \mathbf{t}$. The geometric interpretation of this equation is that $w_k$ functions like a correction of the perspective projection $[x,\ y,\ 1]^T$ to the scaled orthographic projection. The overall objective function for POSIT can be written as:
\begin{align*}
E=\sum_{k=1}^{M}\left(
\left(
\mathbf{Q}_1^T \mathbf{P}_k - w_kx_k
\right)^2+
\left(
\mathbf{Q}_2^T \mathbf{P}_k - w_ky_k
\right)^2
\right)
\end{align*}
where $\mathbf{Q}_1=s[\mathbf{R_1}^T \mathbf{tx}]^T$, $\mathbf{Q}_2=s[\mathbf{R_2}^T \mathbf{ty}]^T$, $\mathbf{P}_k=[\tilde{\mathbf{P}}_k^T, 1]^T$. By taking the derivative of $E$ with respect to $\mathbf{Q}_1, \mathbf{Q}_2$, we can get the least-square solution for $\mathbf{Q}_1, \mathbf{Q}_2$ as
\begin{align*}
\mathbf{Q}_1=\left(
\sum_{k=1}\mathbf{P}_k\mathbf{P}_k^T
\right)^{-1}\left(
\sum_{k=1} w_kx_k\mathbf{P}_k
\right) \\
\mathbf{Q}_2=\left(
\sum_{k=1}\mathbf{P}_k\mathbf{P}_k^T
\right)^{-1}\left(
\sum_{k=1} w_ky_k\mathbf{P}_k
\right)
\end{align*}
\subsubsection{Correspondence Update routine}
SoftPOSIT~\cite{softposit} algorithm maintains a soft assign matrix $\mathbf{M} \in \mathbb{R}^{M\times N},\ $ (M is the number of object points, and N is the number of the image points) in which the element $m_{ij}$ represents the likelihood of model point $i$ matches with image point $j$. Since we have no information of the correspondence, so for each $w_k$, its correction has to be applied for each image points. Then we compute the pixel distance of each corrected image coordinates with the scaled orthographic projection of the $k^{th}$ object point:
\begin{align*}
d_{kj}^2=(\mathbf{Q}_1\mathbf{P}_k-w_k x_j)^2 + (\mathbf{Q}_2\mathbf{P}_k-w_k y_j)^2
\end{align*}
An threshold $\alpha$ is used as the maximum tolerate distance for considering an correspondence. The modified optimization problem is described as:
\begin{align*}
E=\sum_{k=1}^{M}\sum_{j=1}^{N}m_{kj}\left(
\left(
\mathbf{Q}_1\mathbf{P}_k-w_k x_j
\right)^2+
\left(
\mathbf{Q}_2\mathbf{P}_k-w_k y_j
\right)^2
\right)
\end{align*}
where $m_{kj}$ serves as a weight ($0 \leq m_{kj} \leq 1$). This is a weighted least square problem which can be similarly solved. 

Once we solve for the new pose, we do the update of the soft assign matrix $\mathbf{M}$ by:
\begin{itemize}
	\item compute the matching distance $d_{kj}^2$.
	\item update $w_k=\gamma exp(-\beta(d_{kj}^2-\alpha))$ and truncate this to be within $0-1$.
	\item the row and column of $\mathbf{M}$ should be $1$ since one object point only matches to one image point. So re-normalization is necessary: this is done by alternatively normalize along the row direction and then along the column direction using the so called Sinkhorn algorithm.
\end{itemize}

\subsection{Summary of POSIT \& SoftPOSIT}
SoftPOSIT~\cite{softposit} iterates to solve this problem, which is related to the general expectation-maximization (EM) algorithm. So generally speaking, global optimality is not guaranteed. Secondly, SoftPOSIT algorithm is usually very slow since its working condition is more challenging. From the experiments I have done, SoftPOSIT is not very stable. Sometimes it cannot find the correct solution.


\subsection{Iterative P$n$P}
The most famous iterative solution should be~\cite{lu2000fast}. The biggest difference of this method compared with other methods would be classical methods apply optimization using Euler angles which constrain their performance and optimality, while this method does the so-called orthogonal iteration. 

The objective function used in this method is the object-space error:
$$
E = \sum_{i=1}^{N} ||(\mathbf{I}-\mathbf{V}_i)(\mathbf{R}\mathbf{P}_i+\mathbf{t})||^2
$$
It is clear that if we make $\mathbf{R}$ fixed, then the objective function is in the quadratic form of $\mathbf{t}$, which means the optimal $\mathbf{t}$ can be computed as
$$
\mathbf{t}(\mathbf{R})=\frac{1}{N}(\mathbf{I}-\frac{1}{N}\sum_{j}\hat{\mathbf{V}}_j)^{-1} \sum_{j}(\hat{\mathbf{V}}_j-\mathbf{I})\mathbf{R}\mathbf{p}_j
$$
With the equation, the iteration process starts with an initial estimation of the $\mathbf{R}_k$:
\begin{itemize}
	\item compute for the $\mathbf{t}_k$.
	\item compute the transformed point coordiante $\mathbf{q}^k_i=\mathbf{R}_k\mathbf{p}^k_i+\mathbf{t}$.
	\item update $\mathbf{R}_k$ as the minimizer for $\sum_{i=1}^{N}||\mathbf{R}_k\mathbf{p}_i+\mathbf{t}-\hat{\mathbf{V}}_i\mathbf{q}^k_i||^2$, which is a $3$D to $3$D pose estimation problem if we assume $\mathbf{q_i^k}$ is fixed.
	\item iterates this process until a local minimum achieves.
\end{itemize}
This method is proved to be globally convergent, which means given an initial guess, it will definitely converge to a local minimum. However, globally convergency does not mean globally optimal. As can be seen that this method is a nonlinear optimization method, which normally would require a initial value. Any P$n$P solution can be a front-end to provide the initial value.~\cite{lu2000fast} also propose an initial method under weak perspective approximation. Weak perspective projection means the depths of all camera space coordinates are roughly equal to the principle depth and the object is close to the optical axis of the camera:
\begin{align*}
u_i = \frac{1}{s}(\mathbf{R}_1^T\mathbf{p}_i+t_x) \\
v_i = \frac{1}{s}(\mathbf{R}_2^T\mathbf{p}_i+t_y)
\end{align*}
Recall the derivation of the POSIT~\cite{oberkampf1996iterative}, it is exactly the same approximation used in POSIT. The initial value can be obtained by simply using the image coordinates as the $\mathbf{q}_i$ for estimating the initial $\mathbf{R}$ and $\mathbf{t}$. ~\cite{lu2000fast} shows that this is approximately the optimization procedure with $s,\ \mathbf{R},\ \mathbf{t}$ simultaneously.



\bibliography{PnPCites} 
\bibliographystyle{ieeetr}

\end{document}